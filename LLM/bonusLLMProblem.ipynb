{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDo-5ZBc76o2",
        "outputId": "bbe804bd-6756-4462-f53c-51a19d2211dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tokenizers\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.13.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 KB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.4 transformers-4.27.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.4-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.3/70.3 KB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.9/dist-packages (from openai) (2.27.1)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (22.2.0)\n",
            "Installing collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.4 yarl-1.8.2\n"
          ]
        }
      ],
      "source": [
        "!pip install tokenizers\n",
        "!pip install transformers\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import openai\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, pipeline"
      ],
      "metadata": {
        "id": "lgvaVpkZ77jO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder = '/content/drive/MyDrive/CSC401/'\n",
        "with open(folder+'api.txt', 'r') as file:\n",
        "    API_KEY = file.read()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SXj4tIA78XE",
        "outputId": "208a0ac7-4826-4326-bed5-31c62c1381e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = API_KEY\n",
        "message_history = []"
      ],
      "metadata": {
        "id": "IBc3USco79R_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def GPT4_predict(prompt, model='gpt-4', max_token=8000):\n",
        "    message_history.append({\"role\": \"user\", \"content\": f\"{prompt}\"})\n",
        "    \n",
        "    completion = openai.ChatCompletion.create(\n",
        "      model=model,\n",
        "      messages=message_history,\n",
        "      temperature=0.3,\n",
        "      max_tokens=max_token\n",
        "    )\n",
        "\n",
        "    reply_content = completion.choices[0].message.content\n",
        "\n",
        "    message_history.append({\"role\": \"assistant\", \"content\": f\"{reply_content}\"})\n",
        "\n",
        "    return reply_content"
      ],
      "metadata": {
        "id": "OSQXrd3M7-Qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt1 = 'Paul tried to call George on the phone, but he wasnt available. Who was not available?'\n",
        "prompt2 = 'We went to the lake, because a shark had been seen at the ocean beach, so it was a dangerous place to swim. Which was a dangerous place to swim?'\n",
        "prompt3 = 'There are too many deer in the park, so the park service brought in a small pack of wolves. The population should increase over the next few years. Which population will increase?'\n",
        "prompts = [prompt1, prompt2, prompt3]"
      ],
      "metadata": {
        "id": "gqDLZKnm8FVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for p in prompts:\n",
        "  rslt = GPT4_predict(p)\n",
        "  print(rslt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZ27YPXx82nZ",
        "outputId": "d5011286-1c6f-4fc2-d0c3-ce62fda98daa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "George was not available.\n",
            "The ocean beach was a dangerous place to swim.\n",
            "The population of wolves should increase over the next few years.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "message_history = []"
      ],
      "metadata": {
        "id": "WiHVzeWb9v7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt1 = 'Paul tried to call George on the phone, but he wasnt available. Who was not available? Why? Are there any other possibilities?'\n",
        "prompt2 = 'We went to the lake, because a shark had been seen at the ocean beach, so it was a dangerous place to swim. Which was a dangerous place to swim? Why? Are there any other possibilities?'\n",
        "prompt3 = 'There are too many deer in the park, so the park service brought in a small pack of wolves. The population should increase over the next few years. Which population will increase? Why? Are there any other possibilities?'\n",
        "prompts = [prompt1, prompt2, prompt3]"
      ],
      "metadata": {
        "id": "gAD9ge0O9kHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for p in prompts:\n",
        "  rslt = GPT4_predict(f' {p}')\n",
        "  print(rslt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRA7zfcW9dF9",
        "outputId": "63920b9c-83de-42e6-f19c-a68668abf730"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "George was not available. The reason is not mentioned in the given information. There could be various possibilities for George's unavailability, such as being busy, not hearing the phone, having his phone turned off, or being in an area with poor reception.\n",
            "The ocean beach was a dangerous place to swim. The reason is that a shark had been seen there. Other possibilities for the ocean beach being dangerous could include strong currents, high waves, or hazardous marine life. However, based on the given information, the main reason for the danger was the presence of a shark.\n",
            "The population that should increase over the next few years is the wolf population. The reason is that the park service brought in a small pack of wolves to help control the deer population. Other possibilities could include the deer population continuing to increase despite the presence of wolves, or the populations of other animals in the park being affected by the introduction of wolves. However, based on the given information, the main expectation is for the wolf population to increase.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_prompt1 = 'We went to the lake, because a shark had been seen at the ocean beach, so it was a dangerous place to swim. Which was a dangerous place to swim? Why is it dangerous? Why is the counterpart safer? Explain the underlying mechanism in plain words.'\n",
        "new_prompt2 = ' There are too many deer in the park, so the park service brought in a small pack of wolves. The population should increase over the next few years. Which population will increase? Why would it increase? Would there be any decrease with the increase? Explain the underlying mechanism in plain words'\n",
        "new_prompts = [new_prompt1, new_prompt2]"
      ],
      "metadata": {
        "id": "ziJr1h0G9dLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for p in new_prompts:\n",
        "  rslt = GPT4_predict(f' {p}')\n",
        "  print(rslt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4B3I81xNDkPW",
        "outputId": "6a56d172-e797-4a37-d397-725e7eb7841a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The ocean beach was a dangerous place to swim because a shark had been seen there. Sharks can pose a threat to swimmers as they may attack or bite humans, causing serious injuries or even fatalities. In this case, the lake is considered a safer counterpart for swimming because it does not have the presence of sharks. Lakes typically do not have sharks, as they are saltwater creatures and lakes are freshwater bodies. So, by choosing to swim in the lake instead of the ocean beach, the risk of encountering a shark is significantly reduced, making it a safer option.\n",
            "The wolf population is expected to increase over the next few years. The reason for this increase is that the park service brought in a small pack of wolves to help control the deer population. As the wolves find an abundant food source in the form of deer, they are likely to thrive, reproduce, and grow in numbers.\n",
            "\n",
            "With the increase in the wolf population, it is expected that the deer population will decrease. The underlying mechanism is that wolves are natural predators of deer. As the number of wolves increases, they will hunt and consume more deer, which should help to reduce the deer population. This predator-prey relationship is a natural way to maintain balance in an ecosystem, ensuring that no single species becomes too dominant or overpopulated.\n"
          ]
        }
      ]
    }
  ]
}