1. Architecture change and hyperparameters
I modified the strucutre of the classifier a bit by using two layers of FC connected by a RELU. This has been proved to improve the performance of the model over a simple linear layer.

I used the following hyper parameters:
Hyperparam    |Value
--------------|--------------
Hidden_size   |768
Dropout_prob  |0.2
Learning_rate |2e-5
Batch_size    |8
option        |Flexible
seed          |11711
epochs        |10
Use_GPU       |True



Hyperparam        |Value
------------------|------------------
GPU used          |NVIDIA RTX A4000
Runtime per epoch |~2min
GPU memory        |16G
Memory allocated  |8G


Final Accuracy    |Value
------------------|------------------
Training          |0.999
Dev               |0.971


2. Tuning procedure
I started by tuning the option. I found the accuracy to be significantly higher when option is set to flexible (approx 30% accuracy boost), and set option to flexible in all the following tuning. Then, I attempted to adjust the dropout_prob and learning_rate. However, I did not find significant improvement over the development/training set (~ 0.9 train acc). Then I slightly adjusted the classifier architecture by adding an extra layer, then the training accuracy went up to as high as 0.998. I initially suspected this may be attributed to potential overfit. However, the hight accuracy in dev set (0.967) suggested the superiority in the performance. 

After this, I played around with lr and the dropout prob (*2 and * 1/2, respectively), which caused both accuracy to increas by a marginal amount. I then increased both to the one given above, and achieved really good accuracy. I stopped here beaucse of that I found keeping doing so starts to reduce the accuracy (when lr goes above 3e-4 and dropout goes lower than 0.2).

My tuning strategy is to by doing bounded binary search, and assume the final acc is a convex function regarding lr and dropout prob (as well as batch).

I attempted to tune the batch size. I believed it is a good practice so sometime keep the batch size as large as we could do (although setting it too large tends to cause underfit). However, I found anything above 32 is too large and 8 is good enough, thus I did not change it.